{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Fraude en seguros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera que este proyecto sea de utilidad para ayudar a identificar las reclamaciones fraudulentas con anterioridad a los pagos de sus correspondientes siniestros y de este modo contribuir a la disminución de pérdidas para las compañías de seguros en reclamos ilegítimos, incrementando en últimas las utilidades de la compañía, del sector, de la economía y con suerte la disminución de las primas que cobran a sus asegurados, dado los menores gastos de operación.\n",
    "Para lo anterior, se espera de este proyecto, poner a la disposición de las entidades aseguradoras una herramienta que les sirva para identificar reclamaciones que tengan altas probabilidades de haber sido fraudulentas para que de este modo las aseguradoras puedan enfocar sus recursos de investigación y denuncia de fraude en aquellas reclamaciones que a pesar de sus cuantías tienen altos indicios de fraude, para de este modo evitar que se siga normalizando este tipo de actos en el sector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema específico que se desea resolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta que el objetivo de este proyecto es la construcción de un modelo de machine learning que sea capaz de clasificar, y por añadidura encontrar a priori, los casos de fraude que se cometen en las reclamaciones de seguros; es necesario un dataset que contenga información acerca de los fraudes que se cometieron en una compañía de seguros, los cuales hayan sido documentados con sus respectivas etiquetas, comisión o no de hechos fraudulentos, junto a algunas características de sus clientes y del incidente, para con ello poder alimentar y entrenar el modelo de machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida que se va a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se verá más adelante, en el análisis de la variable fraud_reported de este dataset, son más frecuentes los casos en los que no se comete fraude comparados contra los que sí, haciendo más difícil encontrar patrones que nos lleven a una cauterización de los casos fraudulentos más generalizada, dado el bajo número de entradas que tenemos con estas características. Este tipo de problemas y en general el problema de clasificación de fraude, se les conoce como problema de imbalance data, donde un tipo de datos ocurren más frecuentemente comparado con otros. La descripción y el entendimiento adecuado de los datos se hace crucial entonces, pues mediante este se podrá determinar qué variables deben incluirse o no dentro de la construcción del modelo, para que este a su vez haga un mejor trabajo en la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualizacion de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lograr cumplir con el objetivo planteado en este proyecto se tendrá que usar un dataset estadounidense proveído en un reto de Kaggle; el cual contiene 1000 registros de reclamaciones del seguro, sus características, así como una variable label donde nos informa del incurrimiento de fraude o no por parte del cliente. Las entradas de este dataset fueron tomadas desde el 01 de enero de 2015 hasta 01 de marzo del 2015, es decir, corresponde a 2 meses de información.\n",
    "El dataset cuenta con 39 variables entre cualitativas y cuantitativas que describen las características que tiene una póliza, las del incidente que llevó a la reclamación y del cliente que la adquirió dicha póliza. Esta recolecta información del cliente como lo es meses de vinculación, edad, el género, la ocupación, los hobbies, la relación con el beneficiario; de las caracteristicas del incidente como año del auto, la existencia reporte policial, el daño a propiedad, cantidad de testigos presentes, numero de lesionados, entre otras; y las caracteristicas de la póliza información como número de la misma, fecha de vinculación, estado donde la adquirió, prima de la póliza entre otras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medidas de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que estamos trabajando con un problema de imbalance data, la métrica de accuracy/ precisión no es la medida más adecuada para evaluar los resultados de desempeño de nuestro modelo. Esto porque altos niveles de accuracy pueden ser alcanzados por modelos en los que se logre clasificar la mayoría o la totalidad de los datos de la clase que más representantes tiene en el dataset y mis-clasificar o clasificar incorrectamente la totalidad de los datos que menos casos tiene en el dataset. Por ende, y dado que el objetivo de nuestro proyecto es encontrar un modelo que clasifique correctamente la clase fraude reportado; la medida de desempeño de nuestro modelo no puede ser accuracy.\n",
    "Para nuestro caso particular, se utilizarán las medidas de precision, recall y f1 – score porque contribuyen en mejor medida a la evaluación de desempeño del modelo, siendo precisión el porcentaje de los valores clasificados como positivos correctos, para ello se utiliza la formular True positives dividido por la cantidad de True positives y false positives, queriendo decir, que se revisarán los valores que el modelo clasifique como positivo o fraudulento, y se verificará la cantidad de valores en los cuales la predicción correspondió al valor verdadero. Por otro lado la métrica recall, nos muestra una relación en donde podemos observar la cantidad de observaciones que se tuvieron correctas en una clase, dividido por la cantidad de observaciones que pertenecían a dicha clase.  De igual manera, y dado el trade-off que tienen estas dos métricas donde el incremento de una representa el detrimento de la otra, se utilizará la métrica de f1 – score, la cual es sencillamente el valor de la media armónica entre estas dos métricas, precisión y recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se utilizó stratify como método para lidiar con el problema de imbalance data. Para ello; stratify mantiene las mismas proporciones que se encuentra en el dataset original de datos fraudulentos como no fraudulentos en la selección de su muestra para los datos de entrenamiento, test y validación.\n",
    "Finalmente cabe resaltar, que para contrarrestar a los problemas de overfitting que trae consigo el uso del modelo de XGBoost, fue necesario tomar una muestra de los datos del 90% y de las columnas del 80% en su entrenamiento. Estos valores se fueron ajustando y se realizó la iteración con los mismos 3 veces, esto porque valores de muestra del 50 – 80%, y valores donde las columnas eran de 50 -70% presentaron métricas de clasificación muy bajas, haciendo perder la eficacia del modelo XGBoost.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
